---
description: 管理员指南
keywords: docker, 容器, swarm, manager, raft
title: 管理和维护 Docker Engine 集群
aliases:
- /engine/swarm/manager-administration-guide/
---

在运行 Docker Engine 集群时，管理节点（manager nodes）是管理集群和存储集群状态的关键组件。为了正确部署和维护集群，了解管理节点的一些关键特性非常重要。

有关 Docker Swarm 模式和管理节点与工作节点（worker nodes）之间差异的简要概述，请参考 [节点工作原理](how-swarm-mode-works/nodes.md)。

## 在集群中操作管理节点

Swarm 管理节点使用 [Raft 共识算法](raft.md) 来管理集群。您只需要了解 Raft 的一些通用概念即可管理集群。

管理节点的数量没有限制。决定实现多少个管理节点是在性能和容错性之间进行权衡。向集群添加更多管理节点会使集群更具容错性。然而，额外的管理节点会降低写入性能，因为必须有更多节点确认更新集群状态的提议。这意味着更多的网络往返流量。

Raft 要求大多数管理节点（也称为法定人数 quorum）同意对集群的提议更新，例如节点的添加或删除。成员操作受到与状态复制相同的约束。

### 维护管理节点的法定人数

如果集群失去了管理节点的法定人数，集群就无法执行管理任务。如果您的集群有多个管理节点，始终确保超过两个。为了维持法定人数，大多数管理节点必须可用。建议使用奇数个管理节点，因为下一个偶数个管理节点并不会使法定人数更容易维持。例如，无论您有 3 个还是 4 个管理节点，您仍然只能失去 1 个管理节点并维持法定人数。如果您有 5 个或 6 个管理节点，您仍然只能失去两个。

即使集群失去了管理节点的法定人数，现有工作节点上的 swarm 任务仍会继续运行。但是，无法添加、更新或删除集群节点，也无法启动、停止、移动或更新新任务或现有任务。

如果失去管理节点的法定人数，请参阅 [从失去法定人数中恢复](#recover-from-losing-the-quorum) 以获取故障排除步骤。

## 配置管理节点在静态 IP 地址上发布

在初始化集群时，您必须指定 `--advertise-addr` 标志，以便将您的地址发布给集群中的其他管理节点。更多信息请参阅 [以集群模式运行 Docker Engine](swarm-mode.md#configure-the-advertise-address)。由于管理节点是基础设施的稳定组件，您应该为发布地址使用*固定 IP 地址*，以防止机器重启时集群变得不稳定。

如果整个集群重启，每个管理节点随后都获得新的 IP 地址，那么没有任何节点能够联系到现有的管理节点。因此，集群在节点尝试通过旧 IP 地址相互联系时会陷入停顿。

动态 IP 地址对工作节点来说是可以的。

## 添加管理节点以实现容错

您应该在集群中保持奇数个管理节点，以支持管理节点故障。拥有奇数个管理节点可以确保在网络分区时，如果网络被分割成两组，有更高的几率保持法定人数。如果您遇到超过两个网络分区，保持法定人数则无法保证。

| 集群大小 | 多数 | 容错能力 |
|:------------:|:----------:|:-----------------:|
|      1       |     1      |         0         |
|      2       |     2      |         0         |
|    **3**     |     2      |       **1**       |
|      4       |     3      |         1         |
|    **5**     |     3      |       **2**       |
|      6       |     4      |         2         |
|    **7**     |     4      |       **3**       |
|      8       |     5      |         3         |
|    **9**     |     5      |       **4**       |

例如，在一个有 *5 个节点* 的集群中，如果您失去了 *3 个节点*，您就没有法定人数。因此，在您恢复一个不可用的管理节点或使用灾难恢复命令恢复集群之前，您无法添加或删除节点。请参阅 [从灾难中恢复](#recover-from-disaster)。

虽然可以将集群缩小到单个管理节点，但无法降级最后一个管理节点。这确保您保持对集群的访问，并且集群仍然可以处理请求。缩小到单个管理节点是一个不安全的操作，不推荐这样做。如果在降级操作期间最后一个节点意外离开集群，集群将变得不可用，直到您重新启动节点或使用 `--force-new-cluster` 重启。

您使用 `docker swarm` 和 `docker node` 子系统来管理集群成员。有关如何添加工作节点和将工作节点提升为管理节点的更多信息，请参阅 [将节点添加到集群](join-nodes.md)。

### 分布管理节点

除了保持奇数个管理节点外，在放置管理节点时还要注意数据中心拓扑。为了获得最佳容错性，将管理节点分布在最少 3 个可用区中，以支持整组机器的故障或常见的维护场景。如果您在任何这些区域中遇到故障，集群应该保持可用的管理节点法定人数，以处理请求并重新平衡工作负载。

| 集群管理节点 | 分布（在 3 个可用区上） |
|:-------------------:|:--------------------------------------:|
| 3                   |                  1-1-1                 |
| 5                   |                  2-2-1                 |
| 7                   |                  3-2-2                 |
| 9                   |                  3-3-3                 |

### 运行仅管理节点

默认情况下，管理节点也充当工作节点。这意味着调度器可以将任务分配给管理节点。对于小型且非关键的集群，只要您使用 CPU 和内存的资源约束来调度服务，将任务分配给管理节点的风险相对较低。

但是，由于管理节点使用 Raft 共识算法以一致的方式复制数据，它们对资源饥饿很敏感。您应该将集群中的管理节点与可能阻塞集群操作（如集群心跳或领导者选举）的进程隔离。

为了避免干扰管理节点操作，您可以排空管理节点，使其无法作为工作节点：

```console
$ docker node update --availability drain <NODE>
```

当您排空一个节点时，调度器会将节点上运行的任何任务重新分配给集群中的其他可用工作节点。它还防止调度器将任务分配给该节点。

## 添加工作节点以实现负载均衡

[将节点添加到集群](join-nodes.md) 以平衡集群的负载。复制服务任务在时间上尽可能均匀地分布在集群中，只要工作节点符合服务的要求。当将服务限制为仅在特定类型节点上运行时，例如具有特定 CPU 数量或内存量的节点，请记住不符合这些要求的工作节点无法运行这些任务。

## 监控集群健康状况

您可以通过 `/nodes` HTTP 端点以 JSON 格式查询 docker `nodes` API 来监控管理节点的健康状况。更多信息请参阅 [nodes API 文档](/reference/api/engine/version/v1.25/#tag/Node)。

从命令行运行 `docker node inspect <id-node>` 来查询节点。例如，查询节点作为管理节点的可达性：

```console
$ docker node inspect manager1 --format "{{ .ManagerStatus.Reachability }}"
reachable
```

查询节点作为接受任务的工作节点的状态：

```console
$ docker node inspect manager1 --format "{{ .Status.State }}"
ready
```

从这些命令中，我们可以看到 `manager1` 既处于管理节点的 `reachable` 状态，也处于工作节点的 `ready` 状态。

`unreachable` 健康状态意味着该特定管理节点无法从其他管理节点访问。在这种情况下，您需要采取行动恢复不可达的管理节点：

- 重启守护进程，查看管理节点是否恢复为可达状态。
- 重启机器。
- 如果重启和重新启动都不起作用，您应该添加另一个管理节点或将工作节点提升为管理节点。您还需要使用 `docker node demote <NODE>` 和 `docker node rm <id-node>` 干净地从管理节点集合中移除失败的节点条目。

或者，您也可以从管理节点使用 `docker node ls` 获得集群健康的概览：

```console
$ docker node ls
ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
1mhtdwhvsgr3c26xxbnzdc3yp    node05    Accepted    Ready   Active
516pacagkqp2xc3fk9t1dhjor    node02    Accepted    Ready   Active        Reachable
9ifojw8of78kkusuc4a6c23fx *  node01    Accepted    Ready   Active        Leader
ax11wdpwrrb6db3mfjydscgk7    node04    Accepted    Ready   Active
bb1nrq2cswhtbg4mrsqnlx1ck    node03    Accepted    Ready   Active        Reachable
di9wxgz8dtuh9d2hn089ecqkf    node06    Accepted    Ready   Active
```

## 故障排除管理节点

您永远不应该通过从另一个节点复制 `raft` 目录来重启管理节点。数据目录对节点 ID 是唯一的。节点只能使用节点 ID 一次加入集群。节点 ID 空间应该是全局唯一的。

要干净地将管理节点重新加入集群：

1. 使用 `docker node demote <NODE>` 将节点降级为工作节点。
2. 使用 `docker node rm <NODE>` 从集群中移除节点。
3. 使用 `docker swarm join` 以干净的状态将节点重新加入集群。

有关将管理节点加入集群的更多信息，请参阅 [将节点加入集群](join-nodes.md)。

## 强制移除节点

在大多数情况下，您应该在使用 `docker node rm` 命令从集群中移除节点之前关闭节点。如果节点变得不可达、无响应或被破坏，您可以通过传递 `--force` 标志来强制移除节点，而无需关闭它。例如，如果 `node9` 被破坏：

```console
$ docker node rm node9

Error response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed

$ docker node rm --force node9

Node node9 removed from swarm
```

在强制移除管理节点之前，您必须先将其降级为工作角色。如果您降级或移除管理节点，确保您始终有奇数个管理节点。

## 备份集群

Docker 管理节点在 `/var/lib/docker/swarm/` 目录中存储集群状态和管理日志。这些数据包括用于加密 Raft 日志的密钥。没有这些密钥，您无法恢复集群。

您可以使用任何管理节点备份集群。使用以下过程。

1.  如果集群启用了自动锁定，您需要解锁密钥才能从备份中恢复集群。如有必要，检索解锁密钥并将其存储在安全位置。如果您不确定，请阅读 [锁定您的集群以保护其加密密钥](swarm_manager_locking.md)。

2.  在备份数据之前停止管理节点上的 Docker，以便在备份期间没有数据被更改。在管理节点运行时进行备份（"热"备份）是可能的，但不推荐这样做，因为恢复时的结果不太可预测。在管理节点关闭期间，其他节点继续生成不属于此备份的集群数据。

    > [!NOTE]
    > 
    > 确保维持集群管理节点的法定人数。在管理节点关闭期间，如果进一步失去节点，您的集群更容易失去法定人数。您运行的管理节点数量是一个权衡。如果您定期关闭管理节点进行备份，请考虑运行五节点管理集群，以便在备份运行时您可以失去一个额外的管理节点，而不会中断您的服务。

3.  备份整个 `/var/lib/docker/swarm` 目录。

4.  重新启动管理节点。

要恢复，请参阅 [从备份中恢复](#restore-from-a-backup)。

## 从灾难中恢复

### 从备份中恢复

按照 [备份集群](#back-up-the-swarm) 中描述的步骤备份集群后，使用以下过程将数据恢复到新集群。

1.  关闭目标主机上的 Docker。

2.  删除新集群上 `/var/lib/docker/swarm` 目录的内容。

3.  使用备份的内容恢复 `/var/lib/docker/swarm` 目录。

    > [!NOTE]
    > 
    > 新节点使用与旧节点相同的磁盘存储加密密钥。目前无法更改磁盘存储加密密钥。
    >
    > 对于启用了自动锁定的集群，解锁密钥也与旧集群相同，需要解锁密钥来恢复集群。

4.  在新节点上启动 Docker。如有必要，解锁集群。使用以下命令重新初始化集群，以便此节点不会尝试连接到旧集群的一部分且可能不再存在的节点。

    ```console
    $ docker swarm init --force-new-cluster
    ```

5.  验证集群状态是否符合预期。这可能包括应用程序特定的测试，或者简单地检查 `docker service ls` 的输出以确保所有预期的服务都存在。

6.  如果您使用自动锁定，[轮换解锁密钥](swarm_manager_locking.md#rotate-the-unlock-key)。

7.  添加管理节点和工作节点，将您的新集群提升到运行容量。

8.  在新集群上恢复您之前的备份方案。

### 从失去法定人数中恢复

集群对故障具有弹性，可以从任何数量的临时节点故障（机器重启或崩溃重启）或其他临时错误中恢复。但是，如果集群失去了法定人数，它无法自动恢复。现有工作节点上的任务继续运行，但无法执行管理任务，包括扩展或更新服务以及从集群中加入或移除节点。恢复的最佳方法是让丢失的管理节点重新上线。如果无法做到这一点，请继续阅读以了解恢复集群的一些选项。

在有 `N` 个管理节点的集群中，必须始终有法定人数（多数）的管理节点可用。例如，在有五个管理节点的集群中，最少必须有三个处于运行状态并相互通信。换句话说，集群最多可以容忍 `(N-1)/2` 次永久故障，超过此数量就无法处理涉及集群管理的请求。这些类型的故障包括数据损坏或硬件故障。

如果您失去了管理节点的法定人数，您无法管理集群。如果您失去了法定人数，并尝试对集群执行任何管理操作，会发生错误：

```text
Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
```

从失去法定人数中恢复的最佳方法是让失败的节点重新上线。如果无法做到这一点，从这个状态中恢复的唯一方法是从管理节点使用 `--force-new-cluster` 操作。这会移除除执行命令的管理节点之外的所有管理节点。由于现在只有一个管理节点，因此实现了法定人数。提升节点成为管理节点，直到您有所需数量的管理节点。

从要恢复的节点运行：

```console
$ docker swarm init --force-new-cluster --advertise-addr node01:2377
```

当您使用 `--force-new-cluster` 标志运行 `docker swarm init` 命令时，运行该命令的 Docker Engine 成为单节点集群的管理节点，该集群能够管理和运行服务。管理节点具有关于服务和任务的所有先前信息，工作节点仍然是集群的一部分，服务仍在运行。您需要添加或重新添加管理节点，以实现您之前的工作负载分布，并确保您有足够的管理节点来维持高可用性并防止失去法定人数。

## 强制集群重新平衡

通常，您不需要强制集群重新平衡其任务。当您向集群添加新节点时，或者节点在一段时间不可用后重新连接到集群时，集群不会自动为该空闲节点分配工作负载。这是一个设计决策。如果集群为了平衡而定期将任务转移到不同的节点，使用这些任务的客户端会受到干扰。目标是避免干扰正在运行的服务以实现集群中的平衡。当新任务启动时，或者当具有运行任务的节点变得不可用时，这些任务会被分配给不太繁忙的节点。目标是最终平衡，同时最小化对最终用户的干扰。

您可以使用 `--force` 或 `-f` 标志与 `docker service update` 命令一起强制服务在其可用的工作节点之间重新分配其任务。这会导致服务任务重启。客户端应用程序可能会受到干扰。如果您已配置，您的服务使用[滚动更新](swarm-tutorial/rolling-update.md)。

如果您使用的是早期版本，并且您希望在工作节点之间实现均匀的负载平衡且不介意干扰正在运行的任务，您可以通过暂时向上扩展服务来强制您的集群重新平衡。使用 `docker service inspect --pretty <servicename>` 查看服务的配置规模。当您使用 `docker service scale` 时，具有最少任务数量的节点会被定位以接收新的工作负载。您的集群中可能有多个负载不足的节点。您可能需要适度增加服务规模几次才能实现您想要的跨所有节点的平衡。

当负载平衡到您满意的程度时，您可以将服务规模缩小回原始规模。您可以使用 `docker service ps` 来评估您的服务跨节点的当前平衡。