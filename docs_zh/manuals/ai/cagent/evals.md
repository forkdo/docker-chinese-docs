---
title: Evals
description: 使用保存的对话测试你的智能体
keywords: [cagent, evaluations, testing, evals]
weight: 80
---

评估（evals）帮助你跟踪智能体行为随时间的变化。当你将对话保存为评估时，可以稍后重放它，查看智能体是否以不同方式响应。评估衡量一致性，而非正确性——它们告诉你行为是否改变，而不是判断它是对还是错。

## 什么是评估

评估是一段可以重放的已保存对话。当你运行评估时，cagent 会重放用户消息，并将新响应与原始保存的对话进行比较。高分表示智能体行为相似；低分表示行为发生了变化。

如何使用这些信息取决于你保存对话的目的。你可能会保存成功的对话以捕获回归问题，或者保存失败案例以记录已知问题并跟踪它们是否改善。

## 常见工作流

如何使用评估取决于你的目标：

回归测试：保存智能体表现良好的对话。当你稍后进行更改时（升级模型、更新提示、重构代码），运行这些评估。高分表示行为保持一致，这通常是期望的结果。低分表示某些行为发生了变化——检查新行为以判断它是否仍然正确。

跟踪改进：保存智能体挣扎或失败的对话。当你进行改进时，运行这些评估以查看行为如何演变。低分表示智能体现在行为不同，这可能意味着你修复了问题。你需要手动验证新行为是否确实更好。

记录边缘案例：无论质量如何，保存有趣或不寻常的对话。使用它们来了解智能体如何处理边缘案例，以及这种行为是否会随时间变化。

评估衡量行为是否改变。你来判断这种改变是好是坏。

## 创建评估

从交互式会话中保存对话：

```console
$ cagent run ./agent.yaml
```

与你的智能体进行对话，然后将其保存为评估：

```console
> /eval test-case-name
Eval saved to evals/test-case-name.json
```

对话将保存到当前工作目录的 `evals/` 目录中。如果需要，你可以在子目录中组织评估文件。

## 运行评估

运行默认目录中的所有评估：

```console
$ cagent eval ./agent.yaml
```

使用自定义评估目录：

```console
$ cagent eval ./agent.yaml ./my-evals
```

对注册表中的智能体运行评估：

```console
$ cagent eval agentcatalog/myagent
```

示例输出：

```console
$ cagent eval ./agent.yaml
--- 0
First message: tell me something interesting about kil
Eval file: c7e556c5-dae5-4898-a38c-73cc8e0e6abe
Tool trajectory score: 1.000000
Rouge-1 score: 0.447368
Cost: 0.00
Output tokens: 177
```

## 理解结果

对于每个评估，cagent 显示：

- **First message** - 已保存对话中的初始用户消息
- **Eval file** - 正在运行的评估文件的 UUID
- **Tool trajectory score** - 智能体使用工具的相似程度（0-1 范围，越高越好）
- **[ROUGE-1](https://en.wikipedia.org/wiki/ROUGE_(metric)) score** - 响应文本的相似性（0-1 范围，越高越好）
- **Cost** - 此评估运行的成本
- **Output tokens** - 生成的 token 数量

较高的分数表示智能体行为与原始记录的对话更相似。分数为 1.0 表示行为完全相同。

### 分数的含义

**Tool trajectory score** 衡量智能体是否以与原始对话相同的顺序调用相同的工具。较低的分数可能表示智能体找到了不同的方法来解决问题，这不一定错误，但值得调查。

**Rouge-1 score** 衡量响应文本与原始文本的相似程度。这是一个启发式度量——不同的措辞可能仍然是正确的，所以将其作为信号而非绝对真理使用。

### 解释你的结果

接近 1.0 的分数意味着你的更改保持了一致的行为——智能体使用相同的方法并产生相似的响应。这通常是好的；你的更改没有破坏现有功能。

较低的分数意味着行为与保存的对话相比发生了变化。这可能是回归，智能体现在表现更差，也可能是改进，智能体找到了更好的方法。

当分数下降时，检查实际行为以判断它是更好还是更差。评估文件以 JSON 格式存储在你的评估目录中——打开文件查看原始对话。然后使用相同的输入测试你的修改后的智能体以比较响应。如果新响应更好，保存新的对话以替换评估。如果更差，你就发现了一个回归。

分数指导你了解什么发生了变化。你的判断决定这种变化是好是坏。

## 何时使用评估

评估帮助你跟踪行为随时间的变化。它们在升级模型或依赖项时捕获回归问题、记录你想修复的已知失败案例，以及了解边缘案例如何随着迭代而演变方面很有用。

评估不适用于确定哪种智能体配置效果最好——它们衡量与已保存对话的相似性，而非正确性。使用手动测试来评估不同的配置并判断哪种效果更好。

保存值得跟踪的对话。建立一个包含重要工作流、有趣边缘案例和已知问题的集合。进行更改时运行你的评估，查看什么发生了变化。

## 接下来

- 查看 [CLI 参考](reference/cli.md#eval) 了解所有 `cagent eval` 选项
- 学习 [最佳实践](best-practices.md) 以构建高效的智能体
- 查看 [示例配置](https://github.com/docker/cagent/tree/main/examples) 了解不同类型的智能体
